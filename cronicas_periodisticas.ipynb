{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "periodismo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqUUjsz_nTLz"
      },
      "source": [
        "En este ejercicio, vamos a recuperar la información correspondiente a la página \"https://cronicasperiodisticas.wordpress.com/\", específicamente, cada uno de las entradas que corresponde a los cronistas y sus crónicas allí incluidos. \r\n",
        "\r\n",
        "El resultado de este ejercicio será un directorio general donde se guardarán los archivos de cada cronista, y un directorio para cada cronista en el cual se guardarán los textos en formato txt. \r\n",
        "\r\n",
        "La lógica que utilizaremos para recuperar la información será ir desde el artículo más reciente (https://cronicasperiodisticas.wordpress.com/2017/04/08/stop-running/) hasta el primero.\r\n",
        "\r\n",
        "Navegaremos por el sitio a través de las fechas del final de cada entrada. De cada entrada recuperamos la categoría (que es el nombre del cronista), el título y el texto principal. Con la url podremos obtener la clave para el nombre del archivo (ej: Uncategorized/stop-running.txt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHUWWUu_yWOt"
      },
      "source": [
        "\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import requests"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "II31JhodyO54"
      },
      "source": [
        "# El enlace a la primera entrada con una crónica (punto de inicio)\r\n",
        "\r\n",
        "url = \"https://cronicasperiodisticas.wordpress.com/2017/03/31/la-herencia-de-juan-manuel-fangio/\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzSv8MjmyezS"
      },
      "source": [
        "r = requests.get(url)\r\n",
        "r"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y27xpZEayo7d"
      },
      "source": [
        "contenido =  r.content\r\n",
        "print(contenido[:1000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBZZaJaDyzPW"
      },
      "source": [
        "sopa = BeautifulSoup(contenido, 'html.parser')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvIi7AVarpxC"
      },
      "source": [
        "Después de estos pasos, ya tenemos listo el contenido de la página. Ahora, vamos a identificar cómo extraer el título y el texto principal.\r\n",
        "\r\n",
        "El título se encuentra en la etiqueta \\<h2\\> y tiene la clase \"pagetitle\". Así, podemos recuperar el título de la página. Probemos esta idea a continuación:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaISS8kusK-0"
      },
      "source": [
        "titulo = sopa.find_all(\"h2\", class_=\"pagetitle\")[0].get_text()\r\n",
        "print(titulo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ncfcR1AtZSe"
      },
      "source": [
        "¡Éxito!\r\n",
        "\r\n",
        "Lo que hicimos en este punto fue:\r\n",
        "\r\n",
        "1. Buscar todas las coincidencias de `<h2 class=\"pagetitle\">` en la \"sopa\"\r\n",
        "2. Aunque el resultado es solamente uno, debemos sacarlo de la lista. Para eso simplemente indicamos que nos recupere el primer resultado `[0]`.\r\n",
        "3. Finalmente, extraemos solamente el texto del título con la función `get_text()`\r\n",
        "\r\n",
        "El siguiente paso consistirá en recuperar el texto de la entrada.\r\n",
        "\r\n",
        "Al inspeccionar la entrada, vemos que el contenido de la entrada se encuentra en el `<div>` con la clase `\"entry\"`. Así que haremos lo siguiente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smamzdR3uhL-"
      },
      "source": [
        "contenido = [parrafo.text for parrafo in texto.select(\"p\") for texto in sopa.find_all(\"div\", class_=\"entry\")]\r\n",
        "print(contenido)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R75LGThlxJFN"
      },
      "source": [
        "Nuevamente, explico lo que hicimos en esta línea de código:\r\n",
        "\r\n",
        "1. Iteramos a través de todos los elementos que contiene el `div` con la clase `\"entry\"`.\r\n",
        "2. Dentro de cada uno de ellos seleccionamos a los que corresponden a párrafos (`p`), así evitamos que nos regrese otra información que no deseamos.\r\n",
        "3. Extraemos el texto de cada párrafo.\r\n",
        "4. Todo esto queda guardado en una lista en la variable contenido.\r\n",
        "\r\n",
        "Solamente nos queda por recuperar el nombre del cronista, que está marcado como categoría. Para este caso, está dentro de una etiqueta `<a>` y no tiene una clase sino un `rel=\"category tag`. En este caso la recuperación de la información es un poco diferente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMf0OSsIyajz"
      },
      "source": [
        "nombre_autor = [autor for autor in sopa.find_all('a', {\"rel\": \"category tag\"})[0]]\r\n",
        "print(nombre_autor[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xgSeGSH2JKq"
      },
      "source": [
        "En este caso, lo que hicimos fue aprovechar el atributo `rel` para seleccionar la etiqueta `a` correspondiente. La manera de hacerlo es a través de un 'diccionario' `{\"rel\": \"category tag\"}`\r\n",
        "\r\n",
        "Facil ¿no es así?\r\n",
        "\r\n",
        "Ya tenemos entonces nuestros tres elementos principales:\r\n",
        "\r\n",
        "* Título\r\n",
        "* Nombre del autor\r\n",
        "* Texto de la crónica\r\n",
        "\r\n",
        "Ahora, vamos a revisar cómo podremos navegar crónica por crónica, de la más nueva a la más antigua.\r\n",
        "\r\n",
        "Para ello, vamos a necesitar extraer la url del div `<div class=\"leftnav\">` donde se encuentra la etiqueta `a` con el atributo `rel=\"prev\"`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZeqvnXt4VUj"
      },
      "source": [
        "nav = [e['href'] for e in sopa.find_all(\"a\", {\"rel\": \"prev\"})]\r\n",
        "print(nav[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV1swJUOSSjy"
      },
      "source": [
        "Ya con esta información, podemos iniciar la construcción de los archivos. \r\n",
        "\r\n",
        "En primer lugar, demos acceso al Google Drive para que nuestros archivos se guarden allí."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lakUQ0jnM1ed"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eu82wXZmSllo"
      },
      "source": [
        "Ahora, vamos a convertir nuestros scripts en funciones para poder usarlo en el loop más fácilmente y hacer nuestro código más legible y sostenible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKDxOiyM5JLm"
      },
      "source": [
        "import os\r\n",
        "import requests\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "\r\n",
        "def sopa(url):\r\n",
        "  '''\r\n",
        "  Con esta función construimos una sopa de cada url que necesitemos\r\n",
        "  '''\r\n",
        "  r = requests.get(url)\r\n",
        "  c = r.content\r\n",
        "  return BeautifulSoup(c, 'html.parser')\r\n",
        "\r\n",
        "def titulo(url):\r\n",
        "  '''\r\n",
        "  Esta función regresa el título\r\n",
        "  '''\r\n",
        "  return sopa(url).find_all(\"h2\", class_=\"pagetitle\")[0].get_text()\r\n",
        "\r\n",
        "def nombre_autor(url):\r\n",
        "  '''\r\n",
        "  Esta función regresa el nombre del autor\r\n",
        "  '''\r\n",
        "  nom = [autor for autor in sopa(url).find_all('a', {\"rel\": \"category tag\"})[0]]\r\n",
        "  return nom[0]\r\n",
        "\r\n",
        "def contenido(url):\r\n",
        "  '''\r\n",
        "  Con esta función recuperamos el contenido de cada crónica\r\n",
        "  '''\r\n",
        "  parrafos = []\r\n",
        "  for texto in sopa(url).find_all(\"div\", class_=\"entry\"):\r\n",
        "    for parrafo in texto.select(\"p\"):\r\n",
        "      parrafos.append(parrafo.text)\r\n",
        "  return parrafos\r\n",
        "\r\n",
        "\r\n",
        "def cronica(url):\r\n",
        "  '''\r\n",
        "  Esta es la función central que vincula todo.\r\n",
        "  Esta construye cada archivo de texto\r\n",
        "  '''\r\n",
        "  path = \"/content/drive/MyDrive/Colab Notebooks\"\r\n",
        "  autor = nombre_autor(url)\r\n",
        "  os.makedirs(os.path.join(path, autor), exist_ok=True)\r\n",
        "  titul = titulo(url)\r\n",
        "  filename = os.path.join(path, autor, titul.lower().replace(\" \", \"-\").replace(r\"\\xa0\", \"-\").replace(\":\", \"\").replace(\"/\", \"\")+'.txt')\r\n",
        "  try:\r\n",
        "    pathlib.Path.exists(filename)\r\n",
        "    print(\"{} ya existe\".format(titul))\r\n",
        "  except:\r\n",
        "    with open(filename, 'w+', encoding='utf-8') as f:\r\n",
        "      f.write(f\"{autor}\\n\")\r\n",
        "      f.write(f\"{titul}\\n\")\r\n",
        "      for c in contenido(url):\r\n",
        "        f.write(f\"{c}\\n\")\r\n",
        "    print(\"{} ha sido creado\".format(titul))\r\n",
        "    f.close()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-vVcs4BT6t8"
      },
      "source": [
        "Y después de tener nuestras funciones, es momento de ponerlas a correr por todo el sitio.\r\n",
        "\r\n",
        "Para ello tenemos que usar un while loop,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLKlKk5R6Uwv"
      },
      "source": [
        "url = \"https://cronicasperiodisticas.wordpress.com/2017/03/31/la-herencia-de-juan-manuel-fangio/\"\r\n",
        "\r\n",
        "nav = [e['href'] for e in sopa(url).find_all(\"a\", {\"rel\": \"prev\"})]\r\n",
        "cronica(url)\r\n",
        "\r\n",
        "\r\n",
        "while nav:\r\n",
        "  url = nav[0]\r\n",
        "  cronica(url)\r\n",
        "  nav = [e['href'] for e in sopa(url).find_all(\"a\", {\"rel\": \"prev\"})]\r\n",
        "\r\n",
        "  \r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}